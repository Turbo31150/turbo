{
  "version": "10.2",
  "timestamp": "2026-02-19T21:08:21",
  "duration_ms": 81183,
  "phases": {
    "1_health": {
      "name": "1_health",
      "score": 1.0,
      "duration_ms": 16340,
      "details": {
        "nodes": {
          "M1": {
            "status": "ONLINE",
            "models": [
              "qwen/qwen3-30b-a3b-2507"
            ],
            "gpus": 6,
            "vram_gb": 46
          },
          "M2": {
            "status": "ONLINE",
            "models": [
              "deepseek-coder-v2-lite-instruct"
            ],
            "gpus": 3,
            "vram_gb": 24
          },
          "M3": {
            "status": "ONLINE",
            "models": [
              "mistral-7b-instruct-v0.3"
            ],
            "gpus": 1,
            "vram_gb": 8
          },
          "OL1": {
            "status": "ONLINE",
            "models": [
              "qwen3:1.7b"
            ]
          },
          "GEMINI": {
            "status": "ONLINE",
            "models": [
              "gemini-3-pro",
              "gemini-3-flash",
              "gemini-2.5-pro",
              "gemini-2.5-flash"
            ]
          }
        },
        "online": 5,
        "total": 5,
        "gpu": {
          "count": 6,
          "total_vram_mb": 47104,
          "used_vram_mb": 29841,
          "utilization_pct": 63
        }
      },
      "issues": []
    },
    "2_inference": {
      "name": "2_inference",
      "score": 1.0,
      "duration_ms": 64840,
      "details": {
        "tests": [
          {
            "node": "M1",
            "model": "qwen/qwen3-30b-a3b-2507",
            "status": "OK",
            "latency_ms": 12496,
            "output_len": 345,
            "has_think_tags": false,
            "is_empty": false,
            "preview": "Un MoE (Mixture of Experts) est une architecture d'intelligence artificielle qui combine plusieurs m"
          },
          {
            "node": "M2",
            "model": "deepseek-coder-v2-lite-instruct",
            "status": "OK",
            "latency_ms": 1327,
            "output_len": 267,
            "has_think_tags": false,
            "is_empty": false,
            "preview": "Un modèle de moindres obstacles (MoO) est une technique d'apprentissage automatique qui combine plus"
          },
          {
            "node": "M3",
            "model": "mistral-7b-instruct-v0.3",
            "status": "OK",
            "latency_ms": 2542,
            "output_len": 283,
            "has_think_tags": false,
            "is_empty": false,
            "preview": "Un MoE (Métrique d'erreur) dans l'IA est une mesure quantitative utilisée pour évaluer la performanc"
          },
          {
            "node": "OL1",
            "model": "qwen3:1.7b",
            "status": "OK",
            "latency_ms": 560,
            "output_len": 275,
            "preview": "Un MoE (Model of Everything) est un modèle d'IA capable de gérer et de gérer toutes les parties d'un"
          },
          {
            "node": "GEMINI",
            "model": "gemini-3-pro",
            "status": "OK",
            "latency_ms": 47889,
            "output_len": 429,
            "preview": "Un modèle MoE (Mixture of Experts) est une architecture de réseau de neurones qui, au lieu d'utilise"
          }
        ],
        "ok": 5,
        "total": 5
      },
      "issues": []
    },
    "5_agents": {
      "name": "5_agents",
      "score": 1.0,
      "duration_ms": 1,
      "details": {
        "agents": {
          "ia-deep": {
            "model": "opus",
            "tools_count": 9,
            "missing_mcp_tools": []
          },
          "ia-fast": {
            "model": "haiku",
            "tools_count": 7,
            "missing_mcp_tools": []
          },
          "ia-check": {
            "model": "sonnet",
            "tools_count": 8,
            "missing_mcp_tools": []
          },
          "ia-trading": {
            "model": "sonnet",
            "tools_count": 8,
            "missing_mcp_tools": []
          },
          "ia-system": {
            "model": "haiku",
            "tools_count": 8,
            "missing_mcp_tools": []
          },
          "ia-bridge": {
            "model": "sonnet",
            "tools_count": 10,
            "missing_mcp_tools": []
          },
          "ia-consensus": {
            "model": "sonnet",
            "tools_count": 11,
            "missing_mcp_tools": []
          }
        },
        "mismatches": [],
        "mcp_tool_count": 87,
        "agent_count": 7
      },
      "issues": []
    },
    "7_errors": {
      "name": "7_errors",
      "score": 1.0,
      "duration_ms": 0,
      "details": {
        "checks": [
          {
            "check": "extract_thinking_tags",
            "pass": true,
            "result": "La vraie reponse"
          },
          {
            "check": "extract_string_output",
            "pass": true,
            "result": "Direct string output"
          },
          {
            "check": "extract_openai_fallback",
            "pass": true,
            "result": "OpenAI format"
          },
          {
            "check": "mcp_tool_count",
            "count": 87
          },
          {
            "check": "routing_consensus",
            "pass": true,
            "nodes": [
              "M1",
              "M2",
              "OL1",
              "GEMINI"
            ]
          },
          {
            "check": "no_localhost",
            "pass": true
          },
          {
            "check": "agent_tool_refs",
            "pass": true,
            "issues": []
          },
          {
            "check": "commander_routing_consensus",
            "pass": true
          }
        ],
        "passed": 8,
        "total": 8
      },
      "issues": []
    }
  },
  "issues": [],
  "recommendations": [],
  "summary": {
    "phases_run": 4,
    "avg_score": 1.0,
    "critical_issues": 0,
    "warnings": 0
  }
}