{
  "version": "10.2",
  "timestamp": "2026-02-20T08:02:08",
  "duration_ms": 89664,
  "phases": {
    "1_health": {
      "name": "1_health",
      "score": 1.0,
      "duration_ms": 18917,
      "details": {
        "nodes": {
          "M1": {
            "status": "ONLINE",
            "models": [
              "qwen/qwen3-30b-a3b-2507"
            ],
            "gpus": 6,
            "vram_gb": 46
          },
          "M2": {
            "status": "ONLINE",
            "models": [
              "deepseek-coder-v2-lite-instruct"
            ],
            "gpus": 3,
            "vram_gb": 24
          },
          "M3": {
            "status": "ONLINE",
            "models": [
              "mistral-7b-instruct-v0.3"
            ],
            "gpus": 1,
            "vram_gb": 8
          },
          "OL1": {
            "status": "ONLINE",
            "models": [
              "qwen3:1.7b"
            ]
          },
          "GEMINI": {
            "status": "ONLINE",
            "models": [
              "gemini-3-pro",
              "gemini-3-flash",
              "gemini-2.5-pro",
              "gemini-2.5-flash"
            ]
          }
        },
        "online": 5,
        "total": 5,
        "gpu": {
          "count": 6,
          "total_vram_mb": 47104,
          "used_vram_mb": 24664,
          "utilization_pct": 52
        }
      },
      "issues": []
    },
    "2_inference": {
      "name": "2_inference",
      "score": 1.0,
      "duration_ms": 70743,
      "details": {
        "tests": [
          {
            "node": "M1",
            "model": "qwen/qwen3-30b-a3b-2507",
            "status": "OK",
            "latency_ms": 12606,
            "output_len": 357,
            "has_think_tags": false,
            "is_empty": false,
            "preview": "Un MoE (Mixture of Experts) est une architecture d'intelligence artificielle qui utilise plusieurs m"
          },
          {
            "node": "M2",
            "model": "deepseek-coder-v2-lite-instruct",
            "status": "OK",
            "latency_ms": 3792,
            "output_len": 269,
            "has_think_tags": false,
            "is_empty": false,
            "preview": "Un modèle de moindres obstacles (MoO) est une technique d'intelligence artificielle qui combine plus"
          },
          {
            "node": "M3",
            "model": "mistral-7b-instruct-v0.3",
            "status": "OK",
            "latency_ms": 2241,
            "output_len": 236,
            "has_think_tags": false,
            "is_empty": false,
            "preview": "Un MoE (Metric of Evaluation) dans l'IA est une mesure utilisée pour évaluer la performance d'un mod"
          },
          {
            "node": "OL1",
            "model": "qwen3:1.7b",
            "status": "OK",
            "latency_ms": 6417,
            "output_len": 274,
            "preview": "Un MoE (Model of Experience) en IA est un modèle de langage qui simule l'expérience humaine, en repr"
          },
          {
            "node": "GEMINI",
            "model": "gemini-3-pro",
            "status": "OK",
            "latency_ms": 45663,
            "output_len": 449,
            "preview": "Un \"Mixture of Experts\" (MoE) est une architecture de réseau de neurones composée de plusieurs sous-"
          }
        ],
        "ok": 5,
        "total": 5
      },
      "issues": []
    },
    "5_agents": {
      "name": "5_agents",
      "score": 1.0,
      "duration_ms": 2,
      "details": {
        "agents": {
          "ia-deep": {
            "model": "opus",
            "tools_count": 9,
            "missing_mcp_tools": []
          },
          "ia-fast": {
            "model": "haiku",
            "tools_count": 7,
            "missing_mcp_tools": []
          },
          "ia-check": {
            "model": "sonnet",
            "tools_count": 8,
            "missing_mcp_tools": []
          },
          "ia-trading": {
            "model": "sonnet",
            "tools_count": 8,
            "missing_mcp_tools": []
          },
          "ia-system": {
            "model": "haiku",
            "tools_count": 8,
            "missing_mcp_tools": []
          },
          "ia-bridge": {
            "model": "sonnet",
            "tools_count": 10,
            "missing_mcp_tools": []
          },
          "ia-consensus": {
            "model": "sonnet",
            "tools_count": 11,
            "missing_mcp_tools": []
          }
        },
        "mismatches": [],
        "mcp_tool_count": 87,
        "agent_count": 7
      },
      "issues": []
    },
    "7_errors": {
      "name": "7_errors",
      "score": 1.0,
      "duration_ms": 0,
      "details": {
        "checks": [
          {
            "check": "extract_thinking_tags",
            "pass": true,
            "result": "La vraie reponse"
          },
          {
            "check": "extract_string_output",
            "pass": true,
            "result": "Direct string output"
          },
          {
            "check": "extract_openai_fallback",
            "pass": true,
            "result": "OpenAI format"
          },
          {
            "check": "mcp_tool_count",
            "count": 87
          },
          {
            "check": "routing_consensus",
            "pass": true,
            "nodes": [
              "M2",
              "OL1",
              "M3",
              "M1",
              "GEMINI"
            ]
          },
          {
            "check": "no_localhost",
            "pass": true
          },
          {
            "check": "agent_tool_refs",
            "pass": true,
            "issues": []
          },
          {
            "check": "commander_routing_consensus",
            "pass": true
          }
        ],
        "passed": 8,
        "total": 8
      },
      "issues": []
    }
  },
  "issues": [],
  "recommendations": [],
  "summary": {
    "phases_run": 4,
    "avg_score": 1.0,
    "critical_issues": 0,
    "warnings": 0
  }
}